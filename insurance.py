# -*- coding: utf-8 -*-
"""Insurance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fhocpKzoIhjnPc9GPxqnZJ_CJakoopwy
"""

import pandas as pd
import numpy as np
import plotly.graph_objects as go
from plotly.offline import iplot

# Load the dataset
df = pd.read_csv("/content/insurance.csv")

# Check for null values
null_values = df.isnull().sum()

print(null_values)

import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

# Load the dataset
df = pd.read_csv("/content/insurance.csv")

# Identify categorical and numerical columns
categorical_cols = ['sex', 'smoker', 'region']
numerical_cols = ['age', 'bmi', 'children', 'charges']

# Create a column transformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ])

# Fit and transform the data
df_processed = preprocessor.fit_transform(df)

# Convert the processed data back to a DataFrame (optional, but useful for inspection)
df_processed = pd.DataFrame(df_processed, columns=preprocessor.get_feature_names_out())

print(df_processed.head())

df['age_cat'] = np.nan
lst = [df]

for col in lst:
    col.loc[(col['age'] >= 18) & (col['age'] <= 35), 'age_cat'] = 'Young Adult'
    col.loc[(col['age'] > 35) & (col['age'] <= 55), 'age_cat'] = 'Senior Adult'
    col.loc[col['age'] > 55, 'age_cat'] = 'Elder'


labels = df["age_cat"].unique().tolist()
amount = df["age_cat"].value_counts().tolist()

colors = ["#ff9999", "#b3d9ff", " #e6ffb3"]

trace = go.Pie(labels=labels, values=amount,
               hoverinfo='label+percent', textinfo='value',
               textfont=dict(size=20),
               marker=dict(colors=colors,
                           line=dict(color='#000000', width=2)))

data = [trace]
layout = go.Layout(title="Amount by Age Category")

fig = go.Figure(data=data, layout=layout)
iplot(fig, filename='basic_pie_chart')

import pandas as pd
import plotly.graph_objs as go
from plotly import tools
from plotly.offline import iplot

# Load the dataset
df = pd.read_csv("/content/insurance.csv")

# Create age categories
df['age_cat'] = pd.cut(df['age'], bins=[17, 35, 55, 100], labels=['Young Adult', 'Senior Adult', 'Elder'])

# Drop rows with NaN values in 'age_cat' (if any)
df.dropna(subset=['age_cat'], inplace=True)

# Calculate average and median charges for each age category
avg_ya_charge = df["charges"].loc[df["age_cat"] == "Young Adult"].mean()
avg_sa_charge = df["charges"].loc[df["age_cat"] == "Senior Adult"].mean()
avg_e_charge = df["charges"].loc[df["age_cat"] == "Elder"].mean()

med_ya_charge = df["charges"].loc[df["age_cat"] == "Young Adult"].median()
med_sa_charge = df["charges"].loc[df["age_cat"] == "Senior Adult"].median()
med_e_charge = df["charges"].loc[df["age_cat"] == "Elder"].median()

# Create bar plots
average_plot = go.Bar(
    x=['Young Adults', 'Senior Adults', 'Elder'],
    y=[avg_ya_charge, avg_sa_charge, avg_e_charge],
    name='Mean',
    marker=dict(
        color="#F5B041"
    )
)

med_plot = go.Bar(
    x=['Young Adults', 'Senior Adults', 'Elder'],
    y=[med_ya_charge, med_sa_charge, med_e_charge],
    name='Median',
    marker=dict(
        color="#48C9B0"
    )
)

# Create subplots
fig = tools.make_subplots(rows=1, cols=2, specs=[[{}, {}]],
                          subplot_titles=('Average Charge by Age','Median Charge by Age'),
                         shared_yaxes=True, print_grid=False)

# Add traces
fig.append_trace(average_plot, 1, 1)
fig.append_trace(med_plot, 1, 2)

# Update layout
fig.update_layout(showlegend=True, title='Age Charges', xaxis=dict(title="Age Category"), yaxis=dict(title="Patient Charges"), bargap=0.15)

# Plot
iplot(fig, filename='custom-sized-subplot-with-subplot-titles')

"""Understanding KMeans Clustering:
Before We Start Explaining the Elbow Method and KMeans Neighbors.
The elbow method is mostly used in unsupervised learning algorithms to determine the optimal number of clusters that should be used to find specific unknown groups within our population. We used the yellowbrick library to implement a simple elbow method and to determine the appropiate number of clusters in our KMeans algorithm.

Terms to Know:
Cluster Centroids: The cluster centroid is the most representative point of a specific cluster. So, if we decide to find three clusters, we will have three cluster centroid.
Euclidean Distance: Is the distance between two data points and this term is essential when gathering the distance between the cluster centroids and the data points.
Elbow Method: The elbow method is a technique used to choose the most optimal number of clusters. Remember, in Kmeans clustering we add the number of clusters in a manual way, so the elbow method is useful when using Kmeans. Why is it called the Elbow method? Because as more iterations run to find the optimal number of clusters, the line will take the shape of the arm and the optimal number of clusters is the point that is in the elbow part of the arm.
Euclidean Distance Formula:
d(x,y)=√(x1−x2)2+(y1−y2)2


where:
x1
  = X-axis value of data observation

x2
  = X-axis value of the cluster centroid

y1
  = Y-axis value of the data observation.

y2
  = Y-axis value of the cluster centroid.


**Explaining the Elbow Method:**

The elbow method finds the average sum of squares distance between the cluster centroid and the data observations. As the number of cluster increases the average sum of squares decreases. Basically, as the number of clusters increases, the distance between the data points and the centroids decreases as well. Whenever, we see the "elbow" that is a rule of thumb to consider the optimal number of clus
"""

from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(12,8))

# KNears Neighbors
df.head()
original_df = df.copy()
original_df.head()

X = df[["bmi", "charges"]]


# Instantiate the clustering model and visualizer
model = KMeans()
visualizer = KElbowVisualizer(model, k=(2,6))

visualizer.fit(X)    # Fit the data to the visualizer
visualizer.poof()

import pandas as pd
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

kmeans = KMeans(n_clusters=3)
kmeans.fit(X)
# Printing the Centroids
print(kmeans.cluster_centers_)
print(kmeans.labels_)

fig = plt.figure(figsize=(12,8))

plt.scatter(X.values[:,0], X.values[:,1], c=kmeans.labels_, cmap="Set1_r", s=25)
plt.scatter(kmeans.cluster_centers_[:,0] ,kmeans.cluster_centers_[:,1], color='black', marker="x", s=250)
plt.title("Kmeans Clustering \n Finding Unknown Groups in the Population", fontsize=16)
plt.show()

"""Adding more Clusters with Hierarchical Clustering:
Two types of Approaches: </b>
**Agglomerative** (bottom-up): Each observation starts as one cluster. Based on the distance of those clusters (in this case observations) it will merge into one cluster. For instance, let's say observation A (Cluster A) and observation B (Cluster B) are two different clusters that are within a close distance. So in essence, it combines the two nearest clusters into one bigger cluster. Remember, as with K-Means clustering each cluster is represented by the centroid which is the average position of the data points (observations). Then each centroid will merge either with other centroids from other clusters or with individual observations which are considered to be individual clusters.

**Divisive** (Top-Bottom): With this approach we start at the top with one big cluster. The cluster will be partitioned at a point where it splits the big cluster into two big ones and it will run K-means into each of the clusters splitting the data further down. This will get to a point where the observations cannot be split any more since each observation becomes its own cluster. If you want to understand more about K-means look at the previos example at the top of hierarchical clustering. In practice, the divisive method is not used as often as the Agglomerative method.
"""

from sklearn.cluster import AgglomerativeClustering

X = df[["bmi", "charges"]]

agglomerative_clustering = AgglomerativeClustering(n_clusters=4).fit(X)
agglomerative_clustering

agglomerative_clustering.labels_

plt.style.use("Solarize_Light2")

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16,6))

ax1.scatter(X.values[:,0], X.values[:,1], c=agglomerative_clustering.labels_, cmap="Set1_r", s=25)
ax1.set_title("Agglomerative Clustering", fontsize=16)

# Compute linkage matrix on the same data used for clustering in ax1
linked = linkage(X, 'single')  # Use original data X for linkage calculation

dendrogram(linked,
            orientation='top',
            # Do not provide labels here, or use labels derived from linkage on X
            # labels=agglomerative_clustering.labels_,
            distance_sort='descending',
            show_leaf_counts=False,
            ax=ax2)

ax2.set_title("Dendogram on Agglomerative Clustering")

plt.show()

"""**Classification**

**Weight Status vs Charges**
Overweight: Notice how there are two groups of people that get significantly charged more than the other group of overweight people.
Obese: Same thing goes with the obese group, were a significant group is charged more than the other group.
"""

import pandas as pd
import plotly.figure_factory as ff
from plotly.offline import iplot

# Load the dataset
df = pd.read_csv("/content/insurance (1).csv")

# Define weight conditions based on BMI
def weight_condition(bmi):
    if bmi < 18.5:
        return 'Underweight'
    elif 18.5 <= bmi < 25:
        return 'Normal Weight'
    elif 25 <= bmi < 30:
        return 'Overweight'
    else:
        return 'Obese'

df['weight_condition'] = df['bmi'].apply(weight_condition)

# Create the facet grid plot
fig = ff.create_facet_grid(
    df,
    x='age',
    y='charges',
    color_name='weight_condition',
    show_boxes=False,
    marker={'size': 10, 'opacity': 1.0},
    colormap={'Underweight': 'rgb(208, 246, 130)', 'Normal Weight': 'rgb(166, 246, 130)',
             'Overweight': 'rgb(251, 232, 238)', 'Obese': 'rgb(253, 45, 28)'}
)

# Update layout
fig.update_layout(title="Weight Status vs Charges", width=800, height=600, plot_bgcolor='rgb(251, 251, 251)',
                     paper_bgcolor='rgb(255, 255, 255)')

# Plot
iplot(fig, filename='facet - custom colormap')

import pandas as pd
import plotly.graph_objs as go
from plotly.offline import iplot

# Load the dataset
df = pd.read_csv("/content/insurance (1).csv")

# Define weight conditions based on BMI
def weight_condition(bmi):
    if bmi < 18.5:
        return 'Underweight'
    elif 18.5 <= bmi < 25:
        return 'Normal Weight'
    elif 25 <= bmi < 30:
        return 'Overweight'
    else:
        return 'Obese'

df['weight_condition'] = df['bmi'].apply(weight_condition)

# Create DataFrames for obese smokers and non-smokers
obese_smoker = df.loc[(df["weight_condition"] == "Obese") & (df["smoker"] == "yes")]
obese_nonsmoker = df.loc[(df["weight_condition"] == "Obese") & (df["smoker"] == "no")]

# Create scatter traces
trace0 = go.Scatter(
    x=obese_smoker["age"].values,
    y=obese_smoker["charges"].values,
    name='Smokers',
    mode='markers',
    marker=dict(
        size=10,
        color='#DF0101',
        line=dict(
            width=2,
            color='rgb(0, 0, 0)'
        )
    )
)

trace1 = go.Scatter(
    x=obese_nonsmoker["age"].values,
    y=obese_nonsmoker["charges"].values,
    name='Non-Smokers',
    mode='markers',
    marker=dict(
        size=10,
        color='#00FF40',
        line=dict(
            width=2,
        )
    )
)

# Combine data
data = [trace0, trace1]

# Define layout
layout = dict(title='Clear Separation between Obese Smokers and Non-Smokers in Charges',
              yaxis=dict(zeroline=False,
                          title="Patient Charges",
                          titlefont=dict(size=16)),
              xaxis=dict(zeroline=False,
                          title="Age of the Patient",
                          titlefont=dict(
                          size=16))
             )

# Create figure and plot
fig = go.Figure(data=data, layout=layout)
iplot(fig, filename='styled-scatter')

"""**Project-4**"""

# Split test data by group
X_test_smoker = X_test[X_test['smoker'] == 1]
y_test_smoker = y_test[X_test['smoker'] == 1]
X_test_nonsmoker = X_test[X_test['smoker'] == 0]
y_test_nonsmoker = y_test[X_test['smoker'] == 0]

# Train separate models
model_smoker = GradientBoostingRegressor(random_state=42)
model_nonsmoker = GradientBoostingRegressor(random_state=42)

X_train_smoker = X_train_bal[X_train_bal['smoker'] == 1]
y_train_smoker = y_train_bal[X_train_bal['smoker'] == 1]
X_train_nonsmoker = X_train_bal[X_train_bal['smoker'] == 0]
y_train_nonsmoker = y_train_bal[X_train_bal['smoker'] == 0]

model_smoker.fit(X_train_smoker, y_train_smoker)
model_nonsmoker.fit(X_train_nonsmoker, y_train_nonsmoker)

# Predict and evaluate
y_pred_smoker = model_smoker.predict(X_test_smoker)
y_pred_nonsmoker = model_nonsmoker.predict(X_test_nonsmoker)

mae_smoker = mean_absolute_error(y_test_smoker, y_pred_smoker)
mae_nonsmoker = mean_absolute_error(y_test_nonsmoker, y_pred_nonsmoker)
mae_diff = abs(mae_smoker - mae_nonsmoker)

print(f"Non-smoker MAE: {mae_nonsmoker:.2f}")
print(f"Smoker MAE: {mae_smoker:.2f}")
print(f"Difference: {mae_diff:.2f}")

from sklearn.utils.class_weight import compute_sample_weight

sample_weights = compute_sample_weight(class_weight={0:1, 1:2}, y=X_train_bal['smoker'])
model = GradientBoostingRegressor(random_state=42)
model.fit(X_train_bal, y_train_bal, sample_weight=sample_weights)
y_pred_bal = model.predict(X_test)

# Evaluate as before
mae_smoker_bal = MetricFrame(
    metrics=mean_absolute_error,
    y_true=y_test,
    y_pred=y_pred_bal,
    sensitive_features=X_test['smoker']
)
print(mae_smoker_bal.by_group)

# Calculate correction
correction = after_mae_0 - after_mae_1  # If smokers are under-predicted

# Adjust smoker predictions
y_pred_adjusted = y_pred_bal.copy()
y_pred_adjusted[X_test['smoker'] == 1] += correction

# Recalculate MAE difference
mae_smoker_bal_adj = MetricFrame(
    metrics=mean_absolute_error,
    y_true=y_test,
    y_pred=y_pred_adjusted,
    sensitive_features=X_test['smoker']
)
print(mae_smoker_bal_adj.by_group)

print("=== Group-Specific Models ===")
print(f"Non-smoker MAE: {mae_nonsmoker:.2f}")
print(f"Smoker MAE: {mae_smoker:.2f}")
print(f"Difference: {mae_diff:.2f}\n")

print("=== Weighted Loss Model ===")
print(mae_smoker_bal)  # Assuming this MetricFrame contains weighted loss results
print(f"Difference: {abs(mae_smoker_bal.by_group[0] - mae_smoker_bal.by_group[1]):.2f}\n")

print("=== Post-Processing Adjusted Model ===")
print(mae_smoker_bal_adj)  # Assuming this MetricFrame contains post-processed results
print(f"Difference: {abs(mae_smoker_bal_adj.by_group[0] - mae_smoker_bal_adj.by_group[1]):.2f}\n")